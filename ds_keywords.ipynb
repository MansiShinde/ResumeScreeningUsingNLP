{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Creating keywords dictionary from the job description. This dictionary will be used to filter out the resumes suitable for that job. \n",
        "For example, if the job description/job post is about Data Analyst,the dictionary will have all the words relevant to Data Analyst, and it will find for those words in the resume and select it. "
      ],
      "metadata": {
        "id": "i7z4KOSLGp7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYQvFcRyGQeE",
        "outputId": "c4529785-ff1a-4de5-9752-9bfc8584f93b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/libs/base/past/types/oldstr.py:33: DeprecationWarning: invalid escape sequence \\d\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import gensim\n",
        "import numpy\n",
        "import pyLDAvis.gensim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spacy.lang.en import English\n",
        "from spacy import displacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import Phrases\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1x4bsyDGQeI",
        "outputId": "330d8f8a-2878-414f-b108-5077dd120cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/pyldavis/pyLDAvis-2.1.2.tar.gz?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2E1LzNhL2FmODJlMDcwYThhOTZlMTMyMTdjOGYzNjJmOWE3M2U4MmQ2MWFjOGZmZjNhMjU2MTk0NmE5N2Y5NjI2Ni9weUxEQXZpcy0yLjEuMi50YXIuZ3ojc2hhMjU2PTAyMjA0MTIyOWE4ZmE4OGVlZTM1NGJiMTk4OTZmM2U4NzI5M2E4MmY5MGJlOThhZDBkZjFmZGQ0OTRhMGZhMjI= (1.6MB)\n",
            "\u001b[K    100% |################################| 1.6MB 5.5MB/s ta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in ./libraries/nb_env/lib/python3.6/site-packages (from pyLDAvis)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /libs/project (from pyLDAvis)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /libs/project (from pyLDAvis)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /libs/project (from pyLDAvis)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /libs/project (from pyLDAvis)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /libs/base (from pyLDAvis)\n",
            "Requirement already satisfied: numexpr in /base_env/py3-anaconda-base/lib/python3.6/site-packages (from pyLDAvis)\n",
            "Requirement already satisfied: pytest in /base_env/py3-anaconda-base/lib/python3.6/site-packages (from pyLDAvis)\n",
            "Requirement already satisfied: future in /libs/base (from pyLDAvis)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/funcy/funcy-1.15-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzY2Lzg5LzQ3OWRlMGFmYmJmYjk4ZDFjNGI4ODc5MzY4MDg3NjQ2MjczMDAyMDhiYjc3MWZjZDgyMzQwMzY0NWEzNi9mdW5jeS0xLjE1LXB5Mi5weTMtbm9uZS1hbnkud2hsI3NoYTI1Nj1jMjQ3YzNkMDg1ZTAzYTg5OTc0ZTBjOWU4MzMxZTlhNzlkYjM3NjhhMjYzNTU2YmE4OTZkNmM5MmQ2NjViYmEy\n",
            "Requirement already satisfied: pytz>=2017.2 in /libs/project (from pandas>=0.17.0->pyLDAvis)\n",
            "Collecting python-dateutil>=2.7.3 (from pandas>=0.17.0->pyLDAvis)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/python-dateutil/python_dateutil-2.8.1-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2Q0LzcwL2Q2MDQ1MGMzZGQ0OGVmODc1ODY5MjQyMDdhZTg5MDcwOTBkZTBiMzA2YWYyYmNlNWQxMzRkNzg2MTVjYi9weXRob25fZGF0ZXV0aWwtMi44LjEtcHkyLnB5My1ub25lLWFueS53aGwjc2hhMjU2PTc1YmIzZjMxZWE2ODZmMTE5Nzc2MjY5MmE5ZWU2YTc1NTBiNTlmYzZjYTNhMWY0YjVkN2UzMmZiOThlMmRhMmE= (227kB)\n",
            "\u001b[K    100% |################################| 235kB 5.6MB/s ta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /libs/base (from jinja2>=2.7.2->pyLDAvis)\n",
            "Requirement already satisfied: py>=1.4.33 in /base_env/py3-anaconda-base/lib/python3.6/site-packages (from pytest->pyLDAvis)\n",
            "Requirement already satisfied: setuptools in ./libraries/nb_env/lib/python3.6/site-packages (from pytest->pyLDAvis)\n",
            "Requirement already satisfied: six>=1.5 in ./libraries/nb_env/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Running setup.py bdist_wheel for pyLDAvis ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/66/0d/3b/484ceb9c1b38c5cb9e0ee9e18ccbf1f1b26d0ff273e91a2892\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis, python-dateutil\n",
            "  Found existing installation: python-dateutil 2.7.2\n",
            "    Uninstalling python-dateutil-2.7.2:\n",
            "      Successfully uninstalled python-dateutil-2.7.2\n",
            "Successfully installed funcy-1.15 pyLDAvis-2.1.2 python-dateutil-2.8.1\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 20.3.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting paramiko\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/paramiko/paramiko-2.7.2-py2.py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzk1LzE5LzEyNGU5Mjg3YjQzZTZmZjNlYmI5Y2RlYTNlNWU4ZTg4NDc1YTg3M2MwNWNjZGY4YjdlMjBkMmM0MjAxZS9wYXJhbWlrby0yLjcuMi1weTIucHkzLW5vbmUtYW55LndobCNzaGEyNTY9NGYzZTMxNmZlZjJhYzYyOGIwNTA5N2E2MzdhZjM1Njg1MTgzMTExZDRiYzFiNTk3OWJkMzk3YzJhYjdiNTg5OA== (206kB)\n",
            "\u001b[K    100% |################################| 215kB 7.1MB/s ta 0:00:011\n",
            "\u001b[?25hCollecting cryptography>=2.5 (from paramiko)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/cryptography/cryptography-3.3.1-cp36-abi3-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzdjL2I2LzFmM2RkNDhhMjJmY2Q1NmYxOWU2Y2ZhOTVmNzRmZjBhNjRiMDQ2MzA2MzU0ZTFiZDJiOTM2YjdjOWFiNC9jcnlwdG9ncmFwaHktMy4zLjEtY3AzNi1hYmkzLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9Nzg4YTNjOTk0MmRmNWU0MzcxYzE5OWQxMDM4M2Y0NGExMDVkNjdkNDAxZmI0MzA0MTc4MDIwMTQyZjAyMDI0NA== (2.7MB)\n",
            "\u001b[K    100% |################################| 2.7MB 43.8MB/s ta 0:00:01\n",
            "\u001b[?25hCollecting bcrypt>=3.1.3 (from paramiko)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/bcrypt/bcrypt-3.2.0-cp36-abi3-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzUyL2E3LzUxYWI2NDgxYWMzNTU1MTc2OTY0Nzc4ODlkOGFiMjMyMTA2YTBkZGFkZGE2NDJjNTRlNDdhMmFiNDBiOS9iY3J5cHQtMy4yLjAtY3AzNi1hYmkzLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9NjNkNGUzZmY5NjE4OGU1ODk4Nzc5YjYwNTc4NzhmZWNmM2YxMWNmZTZlYzNiMzEzZWEwOTk1NWQ1ODdlYzdhNw== (63kB)\n",
            "\u001b[K    100% |################################| 71kB 44.6MB/s ta 0:00:01\n",
            "\u001b[?25hCollecting pynacl>=1.0.1 (from paramiko)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/pynacl/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzlkLzU3LzJmNWU2MjI2YTY3NGIyYmNiNmRiNTMxZThiMzgzMDc5YjY3OGRmNWIxMGNkYWE2MTBkNmNmMjBkNzdiYS9QeU5hQ2wtMS40LjAtY3AzNS1hYmkzLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9MzBmOWI5NmRiNDRlMDliMzMwNGY5ZWE5NTA3OWIxYjczMTZiMmI0ZjM3NDRmZTNhYWVjY2NkOTVkNTQ3MDYzZA== (961kB)\n",
            "\u001b[K    100% |################################| 962kB 53.1MB/s ta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in ./libraries/nb_env/lib/python3.6/site-packages (from cryptography>=2.5->paramiko)\n",
            "Collecting cffi>=1.12 (from cryptography>=2.5->paramiko)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/cffi/cffi-1.14.4-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzFjLzFhLzkwZmE3ZTdlZTA1ZDkxZDAzMzllZjI2NGJkOGMwMDhmNTcyOTJhYmE0YTkxZWM1MTJhMGJiYjM3OWQxZC9jZmZpLTEuMTQuNC1jcDM2LWNwMzZtLW1hbnlsaW51eDFfeDg2XzY0LndobCNzaGEyNTY9MDYzOGMzYWUxYTBlZGZiNzdjNjc2NWQ0ODdmZWU2MjRkMmIxZWUxYmRmZWZmYzFmMGI1OGM2NGQxNDllN2VlYw== (401kB)\n",
            "\u001b[K    100% |################################| 409kB 54.3MB/s ta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /libs/base (from cffi>=1.12->cryptography>=2.5->paramiko)\n",
            "Installing collected packages: cffi, cryptography, bcrypt, pynacl, paramiko\n",
            "  Found existing installation: cffi 1.11.4\n",
            "    Not uninstalling cffi at /libs/base, outside environment /home/jupyter/libraries/nb_env\n",
            "  Found existing installation: cryptography 2.1.4\n",
            "    Not uninstalling cryptography at /libs/base, outside environment /home/jupyter/libraries/nb_env\n",
            "Successfully installed bcrypt-3.2.0 cffi-1.14.4 cryptography-3.3.1 paramiko-2.7.2 pynacl-1.4.0\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 20.3.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "Collecting gensim\n",
            "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='repository.walmart.com', port=443): Read timed out. (read timeout=15)\",)': /repository/pypi-proxy/packages/gensim/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzJiL2UwL2ZhNjMyNjI1MTY5MjA1NmRjODgwYTY0ZWIyMjExN2UwMzI2OTkwNmJhNTVhNjg2NDg2NGQyNGVjOGI0ZS9nZW5zaW0tMy44LjMtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PWIzNmU2MzMwNDcxMDYxY2ZkNzhhYWQ3NTFlMjRjNmI0ZjU2ZDU3NTY5N2FmMGZiYWI0MjY1NTEyODkyN2QyOTY=\u001b[0m\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/gensim/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzJiL2UwL2ZhNjMyNjI1MTY5MjA1NmRjODgwYTY0ZWIyMjExN2UwMzI2OTkwNmJhNTVhNjg2NDg2NGQyNGVjOGI0ZS9nZW5zaW0tMy44LjMtY3AzNi1jcDM2bS1tYW55bGludXgxX3g4Nl82NC53aGwjc2hhMjU2PWIzNmU2MzMwNDcxMDYxY2ZkNzhhYWQ3NTFlMjRjNmI0ZjU2ZDU3NTY5N2FmMGZiYWI0MjY1NTEyODkyN2QyOTY= (24.2MB)\n",
            "\u001b[K    100% |################################| 24.2MB 55.7MB/s ta 0:00:01   22% |#######                         | 5.6MB 29.9MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /libs/project (from gensim)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading https://repository.walmart.com/repository/pypi-proxy/packages/smart-open/smart_open-4.1.2-py3-none-any.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2VhLzU0LzAxNTI1ODE3YjZmMzE1MzNkMzA4OTY4YjgxNDk5OWY3ZTY2NmIyMjM0ZjM5YTU1Y2JlNWRlN2MxZmY5OS9zbWFydF9vcGVuLTQuMS4yLXB5My1ub25lLWFueS53aGwjc2hhMjU2PTAyNzc4MmI3MGM5YjA4Zjc1OGE1MjBiMzM1YWE5ODEzNGVhNzE0YzRmYjg2MGI1ODQyZTYwNDAwNGUwYzVhMjA= (111kB)\n",
            "\u001b[K    100% |################################| 112kB 46.4MB/s ta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in ./libraries/nb_env/lib/python3.6/site-packages (from gensim)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /libs/project (from gensim)\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-3.8.3 smart-open-4.1.2\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 20.3.3 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install pyLDAvis\n",
        "!pip install paramiko\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRdxh0yuGQeI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=PendingDeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=ResourceWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N-x_8YnGQeJ",
        "outputId": "00b5c359-facc-4900-90cd-70fda447c56f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/jupyter/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jupyter/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDdTPn1AGQeJ"
      },
      "outputs": [],
      "source": [
        "parser = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oArM-w5yGQeJ"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.is_punct:\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('at')\n",
        "        elif token.pos_ == \"ADJ\" or token.pos_ == \"VERB\" or token.pos_ == \"RBR\" or token.pos_ == \"RBS\" or token.pos_ == \"RB\" or token.pos_ == \"RP\":\n",
        "            continue\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JwLmhG-GQeK"
      },
      "outputs": [],
      "source": [
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhBho8ncGQeL"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy import displacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.models import Phrases\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozHb1EcHGQeL",
        "outputId": "5d3405f3-0991-4dba-925b-720e57508847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ran run ran\n",
            "happier happy happier\n",
            "charging charge charging\n"
          ]
        }
      ],
      "source": [
        "for w in ['ran', 'happier', 'charging']:\n",
        "    print(w, get_lemma(w), get_lemma2(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPQTGOe8GQeM"
      },
      "outputs": [],
      "source": [
        "en_stop = set(nltk.corpus.stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6LIhnB-GQeM",
        "outputId": "45e75885-8195-4ac2-a8fb-422b9fd650ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             crawl_timestamp  \\\n",
            "0  2019-02-06 05:26:22 +0000   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.indeed.com/viewjob?jk=fd83355c2b23...   \n",
            "\n",
            "                     job_title            category             company_name  \\\n",
            "0  Enterprise Data Scientist I  Accounting/Finance  Farmers Insurance Group   \n",
            "\n",
            "             city state country   inferred_city inferred_state  ...  \\\n",
            "0  Woodland Hills    CA     Usa  Woodland hills     California  ...   \n",
            "\n",
            "                                     job_description   job_type  \\\n",
            "0  Read what people are saying about working here...  Undefined   \n",
            "\n",
            "  salary_offered job_board  geo        cursor contact_email  \\\n",
            "0                   indeed  usa  1.550000e+15                 \n",
            "\n",
            "   contact_phone_number                           uniq_id html_job_description  \n",
            "0                        3b6c6acfcba6135a31c83bd7ea493b18                       \n",
            "\n",
            "[1 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "raw_data = pd.read_csv(\"/data/ds_test.csv\")\n",
        "raw_data.fillna(\"\", inplace = True)\n",
        "\n",
        "print(raw_data.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_jKWE0DGQeN"
      },
      "outputs": [],
      "source": [
        "def prepare_text(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 3]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYRCo50DGQeN",
        "outputId": "3a9cde31-abc7-4625-8c5b-a7937508fbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8 s, sys: 342 ms, total: 8.34 s\n",
            "Wall time: 8.63 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "job_description_data = []\n",
        "job_requirement_data = []\n",
        "required_qualification_data = []\n",
        "\n",
        "for index, entry in raw_data.iterrows():\n",
        "  #print(entry)\n",
        "  job_description_tokens = prepare_text(str(entry[\"job_description\"]).strip())\n",
        "  #print(job_description_tokens)\n",
        "  #job_requirement_tokens = prepare_text(str(entry[\"job_description\"]).strip())\n",
        "  #required_qualification_tokens = prepare_text(str(entry[\"RequiredQual\"]).strip())\n",
        "  \n",
        "  if not len(job_description_tokens) == 0:\n",
        "    job_description_data.append(job_description_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDDKObS2GQeN",
        "outputId": "2985c61e-9920-4e95-9823-975d518a4a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 27.9 ms, sys: 2.1 ms, total: 30 ms\n",
            "Wall time: 28.8 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "job_description_dictionary = corpora.Dictionary(job_description_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ItssGuCGQeO",
        "outputId": "c3c1f4b0-5224-4e5e-ac65-3ce0d33cb37d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2297 words present in job_description_dictionary.\n",
            "Examples from job_description_dictionary...\n",
            "0 02/05/2019\n",
            "1 absolutely\n",
            "2 action\n",
            "3 aggregation\n",
            "4 also\n",
            "5 amongst\n",
            "6 analysis\n",
            "7 analyst\n",
            "8 analytics\n",
            "9 application\n",
            "10 arima\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "print(\"{} words present in job_description_dictionary.\".format(len(job_description_dictionary)))\n",
        "print(\"Examples from job_description_dictionary...\")\n",
        "for k, v in job_description_dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk90eHA1GQeO",
        "outputId": "f65b3ac8-c7de-4d1b-a8e6-23548088d1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 22.3 ms, sys: 1.03 ms, total: 23.4 ms\n",
            "Wall time: 22.9 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "job_description_bow_corpus = [job_description_dictionary.doc2bow(text) for text in job_description_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i8PugAaGQeO",
        "outputId": "11136818-2aec-4103-edf4-f266edb27e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 17.2 ms, sys: 2.02 ms, total: 19.3 ms\n",
            "Wall time: 18.8 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "job_description_bow_corpus = [job_description_dictionary.doc2bow(text) for text in job_description_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YoNCo1GGQeP"
      },
      "outputs": [],
      "source": [
        "# print(\"Examples from job_description_bow_corpus...\")\n",
        "# for i in range(len(job_description_bow_corpus[10])):\n",
        "#   print(\"Word {} (\\\"{}\\\") appears {} time.\".format(job_description_bow_corpus[10][i][0], job_description_dictionary[job_description_bow_corpus[10][i][0]], job_description_bow_corpus[10][i][1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "946FS5SSGQeP"
      },
      "outputs": [],
      "source": [
        "# print(\"Examples from job_description_bow_corpus...\")\n",
        "# for i in range(len(job_description_bow_corpus[10])):\n",
        "#   print(\"Word {} (\\\"{}\\\") appears {} time.\".format(job_description_bow_corpus[10][i][0], job_description_dictionary[job_description_bow_corpus[10][i][0]], job_description_bow_corpus[10][i][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOY4Ge_qGQeP",
        "outputId": "d6308ed9-bf43-49bd-c398-f215c2be0805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11.8 ms, sys: 0 ns, total: 11.8 ms\n",
            "Wall time: 89.4 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "job_description_tfidf_model = models.TfidfModel(job_description_bow_corpus)\n",
        "job_description_tfidf_corpus = job_description_tfidf_model[job_description_bow_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KpBJ23mGQeP"
      },
      "outputs": [],
      "source": [
        "pickle.dump(job_description_bow_corpus, open('job_description_bow_corpus.pkl', 'wb'))\n",
        "pickle.dump(job_description_tfidf_corpus, open('job_description_tfidf_corpus.pkl', 'wb'))\n",
        "job_description_dictionary.save('job_description_dictionary.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGlVAqKoGQeP"
      },
      "outputs": [],
      "source": [
        "NUM_OF_CORES = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XcFJwD4GQeP",
        "outputId": "03b8edcf-bafb-4f24-b6ec-c2e459ccfaf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 48.9 s, sys: 3.88 s, total: 52.7 s\n",
            "Wall time: 1min 6s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "JOB_DESCRIPTION_NUM_TOPICS = 5\n",
        "job_description_bow_lda_model = gensim.models.LdaMulticore(job_description_bow_corpus, num_topics = JOB_DESCRIPTION_NUM_TOPICS, id2word=job_description_dictionary, passes=100, workers=NUM_OF_CORES)\n",
        "job_description_bow_lda_model.save('job_description_bow_lda_model.gensim')\n",
        "job_description_tfidf_lda_model = gensim.models.LdaMulticore(job_description_tfidf_corpus, num_topics = JOB_DESCRIPTION_NUM_TOPICS, id2word=job_description_dictionary, passes=100, workers=NUM_OF_CORES)\n",
        "job_description_tfidf_lda_model.save('job_description_tfidf_lda_model.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4HcjwqEGQeQ"
      },
      "outputs": [],
      "source": [
        "# job_description_bow_topics = job_description_bow_lda_model.print_topics(num_words=10)\n",
        "# print(\"Job Description Topics (BoW): \")\n",
        "# for idx, topic in job_description_bow_topics:\n",
        "#     print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
        "\n",
        "# job_description_tfidf_topics = job_description_tfidf_lda_model.print_topics(num_words=10)\n",
        "# print(\"\\n\\nJob Description Topics (TF-IDF): \")\n",
        "# for idx, topic in job_description_tfidf_topics:\n",
        "#     print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGhBo-K_GQeQ"
      },
      "outputs": [],
      "source": [
        "job_description_dictionary = gensim.corpora.Dictionary.load('job_description_dictionary.gensim')\n",
        "job_description_bow_corpus = pickle.load(open('job_description_bow_corpus.pkl', 'rb'))\n",
        "job_description_bow_lda_model = gensim.models.ldamodel.LdaModel.load('job_description_bow_lda_model.gensim')\n",
        "job_description_tfidf_corpus = pickle.load(open('job_description_tfidf_corpus.pkl', 'rb'))\n",
        "job_description_tfidf_lda_model = gensim.models.ldamodel.LdaModel.load('job_description_tfidf_lda_model.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAdV2nvNGQeQ",
        "outputId": "ac8754e7-f712-4ab7-aca9-fd4f96a5d931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noun Phrases...\n",
            "--- [Format: Noun Phrase -> Root Text] ---\n",
            "Data science -> science\n",
            "machine learning -> learning\n",
            "optimization models -> models\n",
            "Master’s degree -> degree\n",
            "Machine Learning -> Learning\n",
            "Computer Science -> Science\n",
            "Information Technology -> Technology\n",
            "Operations Research -> Research\n",
            "Statistics -> Statistics\n",
            "Applied Mathematics -> Mathematics\n",
            "Econometrics -> Econometrics\n",
            "Successful completion -> completion\n",
            "one or more assessments -> assessments\n",
            "Python -> Python\n",
            "Spark -> Spark\n",
            "Scala -> Scala\n",
            "R -> R\n",
            "example -> example\n",
            "scikit -> scikit\n",
            "\n",
            "\n",
            "Topics relevant to new document are (BoW): \n",
            "Score: 0.8778071403503418\t Topic: 0.052*\"data\" + 0.035*\"experience\" + 0.017*\"business\" + 0.015*\"team\" + 0.014*\"science\" + 0.012*\"model\" + 0.012*\"skill\" + 0.011*\"machine\" + 0.010*\"years\" + 0.009*\"analytics\"\n",
            "\n",
            "\n",
            "Topics relevant to new document are (TF-IDF): \n",
            "Score: 0.8428539633750916\t Topic: 0.003*\"business\" + 0.002*\"model\" + 0.002*\"analytics\" + 0.002*\"customer\" + 0.002*\"technology\" + 0.002*\"application\" + 0.002*\"development\" + 0.002*\"modeling\" + 0.002*\"language\" + 0.002*\"algorithm\"\n",
            "CPU times: user 74.4 ms, sys: 7.08 ms, total: 81.5 ms\n",
            "Wall time: 77.4 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "new_doc = \"Data science, machine learning, optimization models, Master’s degree in Machine Learning, Computer Science, Information Technology, Operations Research, Statistics, Applied Mathematics, Econometrics, Successful completion of one or more assessments in Python, Spark, Scala, or R, Using open source frameworks (for example, scikit learn, tensorflow, torch)\"\n",
        "document = parser(new_doc)\n",
        "\n",
        "print(\"Noun Phrases...\")\n",
        "print(\"--- [Format: Noun Phrase -> Root Text] ---\")\n",
        "for noun_phrase in document.noun_chunks:\n",
        "  #print(noun_phrase.text, noun_phrase.label_, noun_phrase.root.text)\n",
        "  print(noun_phrase.text + \" -> \" + noun_phrase.root.text)\n",
        "\n",
        "new_doc = prepare_text(new_doc)\n",
        "new_doc_bow = job_description_dictionary.doc2bow(new_doc)\n",
        "new_doc_tfidf = job_description_tfidf_model[new_doc_bow]\n",
        "\n",
        "# print(\"\\nTopics relevant to new document are: \")\n",
        "# print(job_description_bow_lda_model.get_document_topics(new_doc_bow, minimum_probability=0.1))\n",
        "\n",
        "counter = 0\n",
        "print(\"\\n\\nTopics relevant to new document are (BoW): \")\n",
        "for index, score in sorted(job_description_bow_lda_model[new_doc_bow], key=lambda tup: -1*tup[1]):\n",
        "  if counter == 0:\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, job_description_bow_lda_model.print_topic(index, 10)))\n",
        "    highest_score = score\n",
        "    counter = counter + 1\n",
        "  elif highest_score - score <= 0.3:\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, job_description_bow_lda_model.print_topic(index, 10)))\n",
        "    counter = counter + 1\n",
        "  else:\n",
        "    break\n",
        "\n",
        "counter = 0\n",
        "print(\"\\n\\nTopics relevant to new document are (TF-IDF): \")\n",
        "for index, score in sorted(job_description_tfidf_lda_model[new_doc_tfidf], key=lambda tup: -1*tup[1]):\n",
        "  if counter == 0:\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, job_description_tfidf_lda_model.print_topic(index, 10)))\n",
        "    highest_score = score\n",
        "    counter = counter + 1\n",
        "  elif highest_score - score <= 0.3:\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, job_description_tfidf_lda_model.print_topic(index, 10)))\n",
        "    counter = counter + 1\n",
        "  else:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcjSXjS9GQeQ",
        "outputId": "59f783ba-1758-4bfb-949e-4e43b677f351"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.corpora.dictionary.Dictionary at 0x7f74b061fd68>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gensim.corpora.Dictionary.load('job_description_dictionary.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TK9_zM1GQeQ",
        "outputId": "c757f240-0021-41fb-b992-951c3069fc18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<gensim.models.ldamulticore.LdaMulticore at 0x7f74aff98160>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gensim.models.ldamodel.LdaModel.load('job_description_bow_lda_model.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9-UYR3fGQeQ"
      },
      "outputs": [],
      "source": [
        "job_description_dictionary = gensim.corpora.Dictionary.load('job_description_dictionary.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eBUJo7fGQeR"
      },
      "outputs": [],
      "source": [
        "new_doc = prepare_text(\"Data science, machine learning, optimization models, Master’s degree in Machine Learning, Computer Science, Information Technology, Operations Research, Statistics, Applied Mathematics, Econometrics, Successful completion of one or more assessments in Python, Spark, Scala, or R, Using open source frameworks (for example, scikit learn, tensorflow, torch)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DIn9JIEGQeR"
      },
      "outputs": [],
      "source": [
        "def prepare_text(text):\n",
        "\ten_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\ttokens = tokenize(text)\n",
        "\ttokens = [token for token in tokens if len(token) > 3]\n",
        "\ttokens = [token for token in tokens if token not in en_stop]\n",
        "\ttokens = [get_lemma(token) for token in tokens]\n",
        "\treturn tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rul9iU0rGQeR"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.is_punct:\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('at')\n",
        "        elif token.pos_ == \"ADJ\" or token.pos_ == \"VERB\" or token.pos_ == \"RBR\" or token.pos_ == \"RBS\" or token.pos_ == \"RB\" or token.pos_ == \"RP\":\n",
        "            continue\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bPjgPXpGQeR"
      },
      "outputs": [],
      "source": [
        "parser = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PLcxV8ZGQeR"
      },
      "outputs": [],
      "source": [
        "new_doc_bow = job_description_dictionary.doc2bow(new_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlUdcfxrGQeR"
      },
      "outputs": [],
      "source": [
        "job_description_bow_lda_model.get_document_topics(new_doc_bow, minimum_probability=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynUBKYqVGQeR"
      },
      "outputs": [],
      "source": [
        "job_description_bow_lda_model = gensim.models.ldamodel.LdaModel.load('job_description_bow_lda_model.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV14WBFqGQeR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "ds_keywords.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}